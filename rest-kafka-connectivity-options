You've identified a core challenge in distributed stream processing with local state stores â€“ data locality vs. API accessibility. Keeping Kafka Streams and REST services separate has benefits, but it does introduce the problem of data locality. Here's how this is typically addressed in the industry:

1. **API Gateway with Intelligent Routing**: Deploy an API gateway or a routing layer that's aware of the Kafka Streams application's metadata. This gateway can direct requests to the correct pod or instance based on key access patterns. Some popular API gateways include Ambassador, Kong, and Nginx.

2. **Use Interactive Queries**: Kafka Streams provides a feature called Interactive Queries, which allows querying local state stores directly. By using metadata services, you can find out which instance has the data for a particular key and then directly query that instance.

3. **Kafka Streams Metadata Endpoint**: Expose an additional endpoint in each service that provides metadata about which keys it has. The API Gateway can query this metadata or cache it and use it to route requests to the appropriate service instance.

4. **State Store Replication**: While this can introduce overhead, having replicas of state stores can be a strategy to ensure that more than one instance can serve the data. This approach can be more complex to implement, but it ensures that requests can be handled by multiple pods or instances, providing higher availability.

5. **Consistent Hashing**: Implement a consistent hashing mechanism to determine which service instance should handle which keys. This approach ensures that all requests for a particular key are routed to the correct service instance that has the data in its local state store.

6. **Use External Databases**: Instead of relying purely on local state stores, use an external, fast, and scalable store like Redis, Cassandra, or another database. This way, your state is decoupled from the processing pods, and any API service instance can access any piece of data. This strategy trades off the ultra-low latency of local state stores for flexibility and scalability.

7. **Sticky Sessions**: If your use case allows for it, you can use sticky sessions on your load balancer to ensure that a particular client always gets routed to the same service instance. This can be beneficial if a client tends to request the same set of keys.

8. **Heartbeat/Monitoring Service**: Implement a service that regularly checks the health and availability of your Kafka Streams services. If one becomes unavailable, this service can help reroute traffic or alert operators.

In practice, many organizations opt for a combination of the above strategies based on their specific needs, infrastructure, and the nature of their workloads. The goal is to achieve a balance between low latency, high availability, and system maintainability.